{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19ab132",
   "metadata": {},
   "source": [
    "# Prediksi Harga Pangan dengan GNN - Versi Peningkatan\n",
    "\n",
    "Notebook ini bertujuan untuk memprediksi harga pangan menggunakan Graph Neural Networks (GNN) dengan menerapkan berbagai peningkatan berdasarkan praktik terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6844f",
   "metadata": {},
   "source": [
    "## 1. Impor Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83d759",
   "metadata": {},
   "source": [
    "## 2. Memuat dan Pra-pemrosesan Data Awal\n",
    "(Asumsikan data dimuat ke dalam DataFrame `df_harga_pangan`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e12951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Muat data Anda di sini\n",
    "# df_harga_pangan = pd.read_csv('path_to_your_data.csv', parse_dates=['tanggal'])\n",
    "# df_harga_pangan = df_harga_pangan.pivot(index='tanggal', columns='provinsi', values='harga')\n",
    "# df_harga_pangan.ffill(inplace=True) # Isi missing values jika ada\n",
    "\n",
    "# Contoh data dummy jika tidak ada data nyata\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "n_provinces = 34\n",
    "data_dummy = np.random.rand(len(dates), n_provinces) * 10000 + 5000\n",
    "df_harga_pangan = pd.DataFrame(data_dummy, index=dates, columns=[f'Provinsi_{i+1}' for i in range(n_provinces)])\n",
    "\n",
    "print(\"Shape data awal:\", df_harga_pangan.shape)\n",
    "df_harga_pangan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb19127",
   "metadata": {},
   "source": [
    "## 3. Peningkatan Fitur (Feature Engineering) (Poin 6)\n",
    "Menambahkan fitur temporal seperti hari dalam seminggu, bulan, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    if isinstance(df_enhanced.index, pd.DatetimeIndex):\n",
    "        df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
    "        df_enhanced['month'] = df_enhanced.index.month\n",
    "        df_enhanced['day_of_year'] = df_enhanced.index.dayofyear\n",
    "        df_enhanced['week_of_year'] = df_enhanced.index.isocalendar().week.astype(int)\n",
    "    # Tambahkan fitur lain jika perlu, misal: lagged features, rolling statistics\n",
    "    return df_enhanced\n",
    "\n",
    "# Terapkan enhance_features sebelum membuat sekuens\n",
    "# Jika df_harga_pangan adalah harga saja, kita perlu struktur yang berbeda untuk fitur tambahan\n",
    "# Untuk GNN, fitur biasanya per node. Jika fitur temporal ini global, cara integrasinya perlu dipikirkan.\n",
    "# Untuk saat ini, kita asumsikan fitur ini akan digabungkan ke data input node.\n",
    "\n",
    "# Mari kita asumsikan df_harga_pangan adalah DataFrame utama yang akan diproses lebih lanjut\n",
    "# Fitur tambahan ini bisa digunakan saat membuat snapshot X untuk GNN\n",
    "# df_harga_pangan_featured = enhance_features(df_harga_pangan.reset_index().set_index('tanggal')) # Contoh jika tanggal adalah index\n",
    "# print(\"Data dengan fitur tambahan:\")\n",
    "# print(df_harga_pangan_featured.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091e23d",
   "metadata": {},
   "source": [
    "## 4. Normalisasi Data (Poin 4)\n",
    "Normalisasi per provinsi. Menggunakan `fit_transform` pada data latih dan `transform` pada data validasi/tes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting (Poin 1 & 9: Menggunakan 20% akhir untuk tes, 20% sebelumnya untuk validasi)\n",
    "total_samples = len(df_harga_pangan)\n",
    "test_size_ratio = 0.20\n",
    "val_size_ratio = 0.20 # Dari sisa data setelah test\n",
    "\n",
    "test_split_idx = int(total_samples * (1 - test_size_ratio))\n",
    "val_split_idx = int(test_split_idx * (1 - val_size_ratio / (1-test_size_ratio) )) # val_size dari sisa train+val\n",
    "\n",
    "df_train_val = df_harga_pangan.iloc[:test_split_idx]\n",
    "df_test = df_harga_pangan.iloc[test_split_idx:]\n",
    "\n",
    "df_train = df_train_val.iloc[:val_split_idx]\n",
    "df_val = df_train_val.iloc[val_split_idx:]\n",
    "\n",
    "print(f\"Ukuran data Latih: {df_train.shape}\")\n",
    "print(f\"Ukuran data Validasi: {df_val.shape}\")\n",
    "print(f\"Ukuran data Tes: {df_test.shape}\")\n",
    "\n",
    "# Normalisasi\n",
    "scalers = {}\n",
    "df_train_scaled = pd.DataFrame(index=df_train.index)\n",
    "df_val_scaled = pd.DataFrame(index=df_val.index)\n",
    "df_test_scaled = pd.DataFrame(index=df_test.index)\n",
    "\n",
    "for col in df_train.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_train_scaled[col] = scaler.fit_transform(df_train[[col]])\n",
    "    df_val_scaled[col] = scaler.transform(df_val[[col]])\n",
    "    df_test_scaled[col] = scaler.transform(df_test[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "print(\"\\nData latih setelah normalisasi (contoh):\")\n",
    "print(df_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74ce4b",
   "metadata": {},
   "source": [
    "## 5. Membuat Sekuens untuk GNN\n",
    "Membuat sekuens input (X) dan target (y) untuk model GNN.\n",
    "Fitur yang dihasilkan oleh `enhance_features` harus diintegrasikan di sini saat membuat `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, target_data, window_size, prediction_horizon):\n",
    "    X, y = [], []\n",
    "    # Asumsikan input_data dan target_data adalah pd.DataFrame dengan kolom provinsi\n",
    "    # dan index adalah waktu.\n",
    "    \n",
    "    # Untuk fitur tambahan dari enhance_features:\n",
    "    # Jika enhance_features menghasilkan kolom baru di input_data, itu akan otomatis masuk.\n",
    "    # Jika fitur tambahan bersifat global (misal, hari libur), perlu cara lain untuk memasukkannya.\n",
    "    # Saat ini, kita asumsikan fitur node adalah harga historis + fitur dari enhance_features jika ada.\n",
    "    \n",
    "    # Contoh sederhana: fitur node hanya harga historis\n",
    "    # Jika ingin fitur dari enhance_features, pastikan input_data sudah mengandungnya.\n",
    "    # Misal, jika input_data adalah df_harga_pangan_featured.\n",
    "    \n",
    "    num_nodes = input_data.shape[1] # Jumlah provinsi/node\n",
    "    \n",
    "    for i in range(len(input_data) - window_size - prediction_horizon + 1):\n",
    "        # Fitur X: (window_size, num_nodes * num_features_per_node)\n",
    "        # Jika hanya harga: (window_size, num_nodes)\n",
    "        # Jika harga + fitur lain (misal, dari enhance_features):\n",
    "        #   perlu reshape X_slice agar sesuai.\n",
    "        #   Contoh: X_slice = input_data.iloc[i:i + window_size].values # (window_size, num_nodes * num_base_features)\n",
    "        #   Jika ada fitur tambahan, misal 'day_of_week', 'month' yang sama untuk semua node pada satu waktu:\n",
    "        #   additional_feats = enhanced_features_df.iloc[i:i+window_size][['day_of_week', 'month']].values\n",
    "        #   X_slice = np.concatenate([X_slice, np.tile(additional_feats, (1, num_nodes))], axis=1) # Ini perlu penyesuaian\n",
    "        \n",
    "        # Untuk GNN, X biasanya (num_nodes, num_features_per_node) per snapshot.\n",
    "        # Jadi, kita buat list of snapshots.\n",
    "        \n",
    "        # Snapshot X: (num_nodes, window_size) - setiap node memiliki fitur berupa time series window_size hari terakhir.\n",
    "        # Atau, jika fitur lain ditambahkan: (num_nodes, window_size + num_additional_features)\n",
    "        \n",
    "        # Versi sederhana: X adalah harga selama window_size untuk semua provinsi\n",
    "        # Target y adalah harga pada prediction_horizon hari ke depan untuk semua provinsi\n",
    "        \n",
    "        # Untuk GNN, kita perlu satu snapshot X (N, F) dan y (N, T_out) atau (N*T_out)\n",
    "        # Di sini, kita akan buat dataset di mana setiap item adalah satu graph (snapshot)\n",
    "        # X_snapshot: (num_nodes, window_size)\n",
    "        # y_snapshot: (num_nodes, 1) # Prediksi 1 hari ke depan\n",
    "        \n",
    "        # Ambil data untuk window saat ini\n",
    "        window_data = input_data.iloc[i : i + window_size].values # Shape: (window_size, num_nodes)\n",
    "        \n",
    "        # Transpose agar menjadi (num_nodes, window_size) -> ini akan jadi fitur node\n",
    "        X_snapshot_features = window_data.T # Shape: (num_nodes, window_size)\n",
    "        \n",
    "        # Target: harga pada hari ke-(i + window_size + prediction_horizon - 1)\n",
    "        # Shape: (num_nodes, 1)\n",
    "        y_snapshot_target = target_data.iloc[i + window_size + prediction_horizon - 1].values.reshape(-1, 1)\n",
    "        \n",
    "        X.append(X_snapshot_features)\n",
    "        y.append(y_snapshot_target)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 30 # 30 hari terakhir sebagai input\n",
    "prediction_horizon = 1 # Prediksi 1 hari ke depan\n",
    "\n",
    "# Menggunakan data yang sudah dinormalisasi\n",
    "# Untuk fitur tambahan dari enhance_features, perlu digabungkan ke df_train_scaled, df_val_scaled, df_test_scaled\n",
    "# sebelum memanggil create_sequences.\n",
    "# Contoh:\n",
    "# df_train_featured = enhance_features(df_train.reset_index().set_index('tanggal')) # Lakukan pada data asli sebelum scaling\n",
    "# Kemudian scale fitur tambahan ini juga dan gabungkan dengan df_train_scaled\n",
    "# Untuk GNN, fitur node bisa berupa [harga_hist_1, ..., harga_hist_30, day_of_week_t, month_t]\n",
    "# Ini memerlukan modifikasi pada create_sequences untuk menangani fitur node yang lebih kompleks.\n",
    "\n",
    "# Untuk kesederhanaan saat ini, fitur node hanya time series harga (window_size)\n",
    "X_train, y_train = create_sequences(df_train_scaled, df_train_scaled, window_size, prediction_horizon)\n",
    "X_val, y_val = create_sequences(df_val_scaled, df_val_scaled, window_size, prediction_horizon)\n",
    "X_test, y_test = create_sequences(df_test_scaled, df_test_scaled, window_size, prediction_horizon)\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}, y_train: {y_train.shape}\") # (num_samples, num_nodes, num_node_features=window_size)\n",
    "print(f\"Shape X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Shape X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Membuat adjacency matrix (contoh: fully connected graph antar provinsi)\n",
    "num_nodes = df_harga_pangan.shape[1]\n",
    "adj = np.ones((num_nodes, num_nodes)) - np.eye(num_nodes) # Terhubung ke semua kecuali diri sendiri\n",
    "edge_index = torch.tensor(np.array(np.where(adj)), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b67d2d",
   "metadata": {},
   "source": [
    "## 6. Membuat PyTorch Geometric Dataset dan DataLoader (Poin 3)\n",
    "Menggunakan `shuffle=True` untuk DataLoader data latih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(X_data, y_data, edge_idx):\n",
    "    dataset = []\n",
    "    for i in range(X_data.shape[0]):\n",
    "        x = torch.tensor(X_data[i], dtype=torch.float) # Node features (num_nodes, num_node_features)\n",
    "        y_sample = torch.tensor(y_data[i], dtype=torch.float) # Target (num_nodes, 1)\n",
    "        data = Data(x=x, edge_index=edge_idx, y=y_sample)\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_graph_dataset(X_train, y_train, edge_index)\n",
    "val_dataset = create_graph_dataset(X_val, y_val, edge_index)\n",
    "test_dataset = create_graph_dataset(X_test, y_test, edge_index)\n",
    "\n",
    "# Batch size akan di-tune oleh Optuna, ini nilai default\n",
    "batch_size = 64 \n",
    "\n",
    "# Poin 3: shuffle=True untuk training_loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Jumlah batch di train_loader: {len(train_loader)}\")\n",
    "print(f\"Jumlah batch di val_loader: {len(val_loader)}\")\n",
    "print(f\"Jumlah batch di test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbba82",
   "metadata": {},
   "source": [
    "## 7. Definisi Model GNN (Poin 2)\n",
    "Mengurangi kompleksitas model: `hidden_channels` lebih kecil (misal 32-64), `num_layers` 2-3. Meningkatkan `dropout` (misal 0.3-0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_layers, dropout_rate, num_nodes):\n",
    "        super(GCNPredictor, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(num_node_features, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Output layer: dari hidden_channels ke 1 (prediksi harga per node)\n",
    "        self.out_mlp = torch.nn.Linear(hidden_channels, 1) \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "            \n",
    "        # x shape: (num_nodes_in_batch, hidden_channels)\n",
    "        # Kita ingin prediksi per node, jadi langsung ke MLP\n",
    "        x = self.out_mlp(x) # Output shape: (num_nodes_in_batch, 1)\n",
    "        return x\n",
    "\n",
    "# Contoh parameter (akan di-tune oleh Optuna)\n",
    "# num_node_features adalah window_size jika hanya harga historis\n",
    "# Jika ada fitur tambahan, ini akan berubah.\n",
    "# X_train.shape[2] adalah num_node_features\n",
    "# num_nodes adalah X_train.shape[1]\n",
    "\n",
    "# model = GCNPredictor(num_node_features=X_train.shape[2], \n",
    "#                      hidden_channels=64, # Poin 2: Kurangi dari 128-256\n",
    "#                      num_layers=2,       # Poin 2: Kurangi dari default\n",
    "#                      dropout_rate=0.3,   # Poin 2: Tingkatkan dropout\n",
    "#                      num_nodes=X_train.shape[1]) \n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc469c5",
   "metadata": {},
   "source": [
    "## 8. Fungsi Training dan Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # Target y ada di data.y, shape (num_nodes_in_batch, 1)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs # data.num_graphs adalah batch size\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            all_preds.append(out.cpu().numpy())\n",
    "            all_targets.append(data.y.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Reshape jika perlu, tergantung bentuk y (num_nodes, 1)\n",
    "    # all_preds: (total_samples * num_nodes, 1)\n",
    "    # all_targets: (total_samples * num_nodes, 1)\n",
    "    \n",
    "    # Untuk evaluasi RMSE/MAE pada skala asli, perlu inverse transform\n",
    "    # Ini memerlukan scaler per provinsi dan perakitan kembali prediksi ke format (num_samples, num_nodes)\n",
    "    # Untuk sementara, hitung metrik pada data ternormalisasi\n",
    "    \n",
    "    mse_scaled = mean_squared_error(all_targets, all_preds)\n",
    "    mae_scaled = mean_absolute_error(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, mse_scaled, mae_scaled, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106aae8",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning dengan Optuna (Poin 8)\n",
    "Memperluas rentang `batch_size`, `learning_rate`, `weight_decay`. Menambahkan `dropout` dan `num_units_mlp` (di sini `hidden_channels` dan `num_layers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bd97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune (Poin 8)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    hidden_channels = trial.suggest_categorical(\"hidden_channels\", [32, 64, 128]) # Poin 2 & 8\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 3) # Poin 2 & 8\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5) # Poin 2 & 8\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True) # Poin 2 & 8\n",
    "    current_batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]) # Poin 8\n",
    "\n",
    "    # Update DataLoaders dengan batch_size saat ini\n",
    "    current_train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True)\n",
    "    current_val_loader = DataLoader(val_dataset, batch_size=current_batch_size, shuffle=False)\n",
    "\n",
    "    model = GCNPredictor(num_node_features=X_train.shape[2], \n",
    "                         hidden_channels=hidden_channels,\n",
    "                         num_layers=num_layers,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                         num_nodes=X_train.shape[1])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # Poin 2: weight_decay\n",
    "    \n",
    "    # Poin 5: Loss training (mix MSE+MAE)\n",
    "    # Untuk kesederhanaan, kita gunakan MSE. Bisa diganti dengan custom loss.\n",
    "    criterion = torch.nn.MSELoss() \n",
    "\n",
    "    n_epochs = 40 # Sesuai info awal, bisa ditingkatkan\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 10 # Early stopping\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, current_train_loader, optimizer, criterion)\n",
    "        val_loss, _, _, _, _ = evaluate(model, current_val_loader, criterion)\n",
    "        \n",
    "        trial.report(val_loss, epoch) # Untuk pruning Optuna\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    return best_val_loss\n",
    "\n",
    "# Poin 8: Perluas jumlah trial\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=50) # Tingkatkan n_trials dari 10\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (Validation Loss): {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Latih model terbaik dengan parameter terbaik pada gabungan train+val, lalu evaluasi di test\n",
    "best_params = trial.params\n",
    "final_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "final_model = GCNPredictor(num_node_features=X_train.shape[2],\n",
    "                           hidden_channels=best_params['hidden_channels'],\n",
    "                           num_layers=best_params['num_layers'],\n",
    "                           dropout_rate=best_params['dropout_rate'],\n",
    "                           num_nodes=X_train.shape[1])\n",
    "\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "print(\"\\nTraining final model...\")\n",
    "# Latih untuk jumlah epoch yang sama atau lebih banyak, tanpa early stopping pada val set\n",
    "# atau dengan early stopping pada subset train_val jika diinginkan\n",
    "n_final_epochs = 60 # Bisa disesuaikan\n",
    "for epoch in range(n_final_epochs):\n",
    "    train_loss = train_epoch(final_model, final_train_loader, optimizer, criterion)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_final_epochs}, Train Loss: {train_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3e236",
   "metadata": {},
   "source": [
    "## 10. Evaluasi Model Terbaik pada Data Tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a508cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_scaled, test_mse_scaled, test_mae_scaled, preds_scaled, targets_scaled = evaluate(final_model, test_loader, criterion)\n",
    "\n",
    "print(f\"Test Loss (Scaled): {test_loss_scaled:.6f}\")\n",
    "print(f\"Test MSE (Scaled): {test_mse_scaled:.6f}\")\n",
    "print(f\"Test MAE (Scaled): {test_mae_scaled:.6f}\")\n",
    "\n",
    "# Inverse transform untuk mendapatkan metrik pada skala asli\n",
    "# Preds_scaled dan targets_scaled memiliki shape (total_test_samples * num_nodes, 1)\n",
    "# Perlu di-reshape ke (total_test_samples, num_nodes) sebelum inverse transform per kolom (provinsi)\n",
    "\n",
    "num_test_samples = X_test.shape[0]\n",
    "num_nodes = X_test.shape[1]\n",
    "\n",
    "preds_scaled_reshaped = preds_scaled.reshape(num_test_samples, num_nodes)\n",
    "targets_scaled_reshaped = targets_scaled.reshape(num_test_samples, num_nodes)\n",
    "\n",
    "preds_original_scale = pd.DataFrame(columns=df_test_scaled.columns[:num_nodes]) # Sesuaikan jika kolom berbeda\n",
    "targets_original_scale = pd.DataFrame(columns=df_test_scaled.columns[:num_nodes])\n",
    "\n",
    "for i, col_name in enumerate(df_test_scaled.columns[:num_nodes]): # Ambil nama kolom dari df_test_scaled\n",
    "    scaler = scalers[col_name]\n",
    "    preds_original_scale[col_name] = scaler.inverse_transform(preds_scaled_reshaped[:, i].reshape(-1, 1)).flatten()\n",
    "    targets_original_scale[col_name] = scaler.inverse_transform(targets_scaled_reshaped[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Hitung RMSE dan MAE pada skala asli\n",
    "rmse_original = np.sqrt(mean_squared_error(targets_original_scale.values.flatten(), preds_original_scale.values.flatten()))\n",
    "mae_original = mean_absolute_error(targets_original_scale.values.flatten(), preds_original_scale.values.flatten())\n",
    "\n",
    "print(f\"\\nTest RMSE (Original Scale): {rmse_original:.2f}\")\n",
    "print(f\"Test MAE (Original Scale): {mae_original:.2f}\")\n",
    "\n",
    "# Visualisasi contoh prediksi vs aktual untuk satu provinsi\n",
    "province_to_plot = df_harga_pangan.columns[0] # Ambil provinsi pertama sebagai contoh\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(targets_original_scale.index, targets_original_scale[province_to_plot], label='Actual Prices')\n",
    "plt.plot(preds_original_scale.index, preds_original_scale[province_to_plot], label='Predicted Prices')\n",
    "plt.title(f'Price Prediction for {province_to_plot}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73d76e",
   "metadata": {},
   "source": [
    "## 11. Catatan Tambahan dan Pertimbangan Lanjutan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9012af",
   "metadata": {},
   "source": [
    "### Poin 5: Loss & Skala Output\n",
    "- **Verifikasi Loss Weighting**: Jika menggunakan loss campuran (MSE+MAE), pastikan pembobotannya benar mengingat MSE bersifat kuadratik dan MAE linear.\n",
    "- **Evaluasi Akhir**: Pastikan metrik evaluasi akhir (RMSE/MAE pada skala asli) tidak terpengaruh oleh bias dari proses scaling/unscaling. Periksa apakah `inverse_transform` dilakukan dengan benar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfc0f3",
   "metadata": {},
   "source": [
    "### Poin 7: Arsitektur & Temporal Awareness\n",
    "Model GCN saat ini memproses setiap snapshot secara independen. Untuk menangkap dependensi temporal antar snapshot secara lebih eksplisit, pertimbangkan:\n",
    "- **GCN + RNN**: Menggabungkan output GCN dari setiap snapshot sebagai input ke lapisan RNN (GRU/LSTM).\n",
    "- **Temporal GNN**: Menggunakan arsitektur GNN yang dirancang khusus untuk data sekuensial graf, seperti:\n",
    "    - CTGCN (Temporal Graph Convolutional Network)\n",
    "    - STGCN (Spatio-Temporal Graph Convolutional Network)\n",
    "    - TGCN (Temporal Graph Convolutional Network - varian lain)\n",
    "- **GCNConv + TimeConv**: Menggunakan lapisan konvolusi temporal (misalnya, Temporal Convolutional Network - TCN) setelah GCNConv untuk memproses dimensi waktu dari fitur node.\n",
    "- **Graph Transformer**: Menggunakan mekanisme attention (misalnya, Transformer) untuk menangkap dependensi spasial dan temporal.\n",
    "\n",
    "Implementasi arsitektur ini akan memerlukan perubahan signifikan pada definisi model dan mungkin juga pada cara data disiapkan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361a214",
   "metadata": {},
   "source": [
    "### Poin Tambahan:\n",
    "- **Walk-Forward Validation (Poin 9)**: Pemisahan data saat ini sudah lebih baik dengan mengambil blok waktu yang berurutan untuk train, val, dan test. Untuk evaluasi yang lebih robust pada data time series, pertimbangkan skema validasi walk-forward atau rolling window yang lebih ketat, di mana model dilatih ulang secara periodik dengan data baru.\n",
    "- **Fitur Eksternal (Poin 6)**: Integrasi fitur eksternal (hari libur, peristiwa ekonomi, cuaca) dapat meningkatkan performa. Pastikan fitur ini diselaraskan dengan benar secara temporal dan diintegrasikan ke dalam fitur node atau sebagai input tambahan ke model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
